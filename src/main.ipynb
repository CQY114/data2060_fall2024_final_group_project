{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA 2060 - Fall 2024 - Final Project - Team YYDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Gaussian Naive Bayes classifier\n",
    "\n",
    "The **Gaussian Naive Bayes (GNB)** classifier is a variant of the *Naive Bayes* algorithm designed for classification problems where the features are continuous rather than binary.\n",
    "It assumes that the features for each class are **independent** and **follow a Gaussian (normal) distribution**.\n",
    "These strong assumptions simplify computations, particularly in high-dimensional data scenarios.\n",
    "\n",
    "This simplicity and efficiency make GNB computationally attractive.\n",
    "However, real-world datasets often violate the assumptions of feature independence and Gaussian distribution, which can lead to suboptimal performance.\n",
    "Furthermore, GNB is sensitive to outliers, as they can disproportionately affect the mean and variance estimates of the Gaussian distribution.\n",
    "\n",
    "In summary, the Gaussian Naive Bayes classifier trades off some predictive accuracy for computational efficiency by relying on strong assumptions about the data.\n",
    "It is best suited for tasks where efficiency and scalability are prioritized over maximum predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Representation**\n",
    "Suppose we have a dataset with $n$ samples, containing $d$ continuous features and a categorical target of $s$ classes.\n",
    "We will view the features as a $d$-dimensional continuous random vector $X=(X_1,\\ldots,X_d)$ and the target as a discrete random variable $Y$ taking value in $\\{1,\\ldots,s\\}$.\n",
    "We want to find a classifier $h\\colon\\mathcal{X}\\to\\mathcal{Y}$, where $\\mathcal{X}=\\mathbb{R}^d$ and $\\mathcal{Y}=\\{1,\\ldots,s\\}$, by estimating the conditional distribution of $Y$ given data $\\left(X^{(1)}=x^{(1)},\\ldots,X^{(n)}=x^{(n)}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *prior*\n",
    "The prior class distribution is given by $p_c=P(Y=c)$ for $c=1,\\ldots,s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *likelihood*\n",
    "By our assumption, the feature vector $X$ follows a $d$-dimensional jointly normal distribution with its components mutually independent.\n",
    "The ***probability density function (PDF)*** of a univariate normal random variable, parametrized by its mean $\\mu$ and variance $\\sigma^2$, is given by\n",
    "$$f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}.$$\n",
    "The joint density function of independent normal random variables can then be obtained by simply taking the product of their individual densities:\n",
    "$$f(x)=f(x_1,\\ldots,x_d)=\\prod_{k=1}^d f(x_k).$$\n",
    "Let $X_{c,k}$ denote the normal random variable that represents the $k$-th feature of a sample in class $c$.\n",
    "Let the parameter tuple $\\left(\\mu_{c,k},\\sigma_{c,k}^2\\right)$ denote the mean and variance of $X_{c,k}$.\n",
    "The (conditional) likelihood of our features can be written as the conditional PDF\n",
    "$$f_{X|Y}(x|c)=f_{X|Y}(x_1,\\ldots,x_d|Y=c)=\\prod_{k=1}^d f_{X_k|Y}(x_k|Y=c),$$\n",
    "where\n",
    "$$f_{X_k|Y}(x_k|c)=\\frac{1}{\\sqrt{2\\pi\\sigma_{c,k}^2}}\\exp\\left\\{-\\frac{(x-\\mu_{c,k})^2}{2\\sigma_{c,k}^2}\\right\\}$$\n",
    "for $c=1,\\ldots,s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *joint distribution*\n",
    "The joint distribution of features and the target cannot be written as a single density.\n",
    "However, we can write one PDF for each of the values $Y$ can take, i.e., for each $c=1,\\ldots,s$ the joint density is given by\n",
    "$$f_{X,Y}(x,c)=P(Y=c)\\cdot f_{X|Y}(x|Y=c).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *marginal distribution*\n",
    "The marginal distribution of $X$ is given by summing the class-wise distributions over all values that $Y$ can take, i.e.\n",
    "$$f_X(x)=\\sum_{c=1}^s f_{X,Y}(x,c).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *posterior*\n",
    "The posterior of $Y$ conditioned on $X=x$ is given by\n",
    "$$P(Y=c|X=x)=\\frac{f_{X,Y}(x,c)}{f_X(x)}=\\frac{P(Y=c)\\cdot f_{X|Y}(x|Y=c)}{f_X(x)}$$\n",
    "for $c=1,\\ldots,s$ and $x\\in\\mathbb{R}^d$.\n",
    "Note that this representation resembles the ***Bayes' theorem***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loss**\n",
    "We can define a cross entropy loss since we are predicting a probability for each class.\n",
    "For a particular sample $i$, suppose we have predicted $\\hat{p}^{(i)}=\\left(\\hat{p}_1^{(i)},\\ldots,\\hat{p}_s^{(i)}\\right)$ and the true label is $y_i$.\n",
    "Then its loss is given by $L_i=-\\log\\hat{p}_{y_i}^{(i)}$.\n",
    "The overall loss on the whole dataset is then given by\n",
    "$$L=\\frac{1}{n}\\sum_{i=1}^n L_i=-\\frac{1}{n}\\sum_{i=1}^n\\log\\hat{p}_{y_i}^{(i)}.$$\n",
    "However, since a loss is not necessary for training a generative model like GNB classifier, this loss serves for an alternative evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Optimizer**\n",
    "The key technique for finding the best classifier is called ***maximum likelihood estimation (MLE)***, which we will use in two situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *prior*\n",
    "The prior class distribution $p_c=P(Y=c)$ can be estimated with the empirical class distribution by computing\n",
    "$$\\hat{p}_c=\\hat{P}(Y=c)=\\frac{n_c}{n}=\\frac{1}{n}\\sum_{i=1}^n 1_{\\{y_i=c\\}}$$\n",
    "for $c=1,\\ldots,s$.\n",
    "Here $n_c$ denotes the number of samples in class $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *likelihood*\n",
    "To estimate the likelihood for each feature given a class, it is sufficient to estimate the mean and variance for each feature and each class, i.e. we will estimate\n",
    "$$\\hat{\\mu}_{c,k}=\\bar{x}_{c,k}=\\frac{1}{n_c}\\sum_{i=1}^{n_c}x_{c,k}^{(i)}$$\n",
    "and\n",
    "$$\\hat{\\sigma}_{c,k}^2=S_{c,k}^2=\\frac{1}{n_c}\\sum_{i=1}^{n_c}\\left(x_{c,k}^{(i)}-\\bar{x}_{c,k}\\right)^2$$\n",
    "for $c=1,\\ldots,s$ and $k=1,\\ldots,d$.\n",
    "Note that there are $2sd$ parameters in total to be estimated, and that we ignore the degree of freedom (dof) in a machine learning context, so the denominator in computing the variance is $n_c$ instead of $n_c-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Algorithm Summary**\n",
    "Suppose we are given a dataset $\\left\\{\\left(x^{(i)},y^{(i)}\\right)\\right\\}_{i=1}^n$, where $x^{(i)}=\\left(x_1^{(i)},\\ldots,x_d^{(i)}\\right)$. The algorithm consists of a training process and a predicting process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *training*\n",
    "We train the model by estimating all the parameters we need for making predictions.\n",
    "1. Estimate the prior: $\\hat{p}_c$ for $c=1,\\ldots,s$.\n",
    "2. Estimate mean and variance for the likelihood: $\\hat{\\mu}_{c,k}$ and $\\hat{\\sigma}_{c,k}^2$ for $c=1,\\ldots,s$ and $k=1,\\ldots,d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *predicting*\n",
    "We use our estimated (thus known and fixed) parameters to make predictions.\n",
    "Given one sample of data $x=(x_1,\\ldots,x_d)$, we will follow the steps as shown below while striving to replace `for` loops with vectorized computations.\n",
    "1. For $c=1,\\ldots,s$ and $k=1,\\ldots,d$, compute the log-likelihood:\n",
    "$$\\log f_{X_k|Y}(x_k|c)=-\\frac{1}{2}\\log\\left(2\\pi\\hat{\\sigma}_{c,k}^2\\right)-\\frac{(x-\\hat{\\mu}_{c,k})^2}{2\\hat{\\sigma}_{c,k}^2}$$\n",
    "2. Convert the prior into log space.\n",
    "3. For $c=1,\\ldots,s$, sum up the log-likelihood over $k$ with the log-prior added to it (denoted by $l_c$):\n",
    "$$l_c=\\log\\hat{p}_c+\\sum_{k=1}^d\\log f_{X_k|Y}(x_k|c)=\\log\\left(\\hat{p}_c\\cdot\\prod_{k=1}^d f_{X_k|Y}(x_k|c)\\right)=\\log\\left(\\hat{p}_c\\cdot f_{X|Y}(x|c)\\right)=\\log f_{X,Y}(x,c).$$\n",
    "4. Predict the class with the highest value of $l_c$:\n",
    "$$\\hat{y}=\\argmax_{c=1,\\ldots,s}l_c.$$\n",
    "Note that step 4 works because\n",
    "- $f_X(x)$ is a normalizing constant independent of $c$, so the posterior $P(Y=c|X=x)$ is proportional to $f_{X,Y}(x,c)$.\n",
    "- the log function is monotonically increasing, so applying it to an array does not change the maximizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *loss*\n",
    "In the case where we want to compute the loss from predicting one sample $(x,y)$, we will follow the steps as shown below.\n",
    "1. For $c=1,\\ldots,s$, convert $l_c$ back out of log space by exponentiating it:\n",
    "$$f_{X,Y}(x,c)=e^{l_c}.$$\n",
    "2. Compute the marginal likelihood of $x$:\n",
    "$$f_X(x)=\\sum_{c=1}^s f_{X,Y}(x,c).$$\n",
    "3. Compute the predicted probability for the true class $y$:\n",
    "$$\\hat{P}(Y=y|X=x)=\\frac{f_{X,Y}(x,y)}{f_X(x)}.$$\n",
    "4. Apply the cross-entropy loss function:\n",
    "$$L=-\\log\\hat{P}(Y=y|X=x).$$\n",
    "The loss from predicting the whole dataset can then be obtained by simply taking the mean over the losses from all samples computed the same way as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normal_log_pdf(x: int | float | np.ndarray,\n",
    "                   mu: int | float | np.ndarray,\n",
    "                   sig2: int | float | np.ndarray):\n",
    "    \"\"\"\n",
    "    Computes the natural log of the probability density function of\n",
    "    a normal distribution with mean `mu` and variance `sig2` evaluated at `x`;\n",
    "    Note: can be applied to a NumPy array, in which case the shapes of\n",
    "    all arguments are expected to conform or be able to broadcast\n",
    "\n",
    "    Args:\n",
    "        `x`: the value(s) at which the pdf is evaluated\n",
    "        `mu`: mean of the normal distribution\n",
    "        `sig2`: variance of the normal distribution\n",
    "\n",
    "    Returns: log-likelihood of observing `x` under the given distribution\n",
    "    \"\"\"\n",
    "    return (-np.log(2*np.pi*sig2) - (x-mu)**2/sig2) / 2\n",
    "\n",
    "class GaussianNaiveBayes:\n",
    "    def __init__(self, var_smoothing: float=1e-9):\n",
    "        \"\"\"\n",
    "        Initializes the Gaussian Naive Bayes classifier\n",
    "        \"\"\"\n",
    "        self.var_smoothing = var_smoothing\n",
    "        self.n_classes = None\n",
    "        self.priors = None\n",
    "        self.mean = None\n",
    "        self.var = None\n",
    "\n",
    "    def train(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Trains the classifier by estimating and storing the following attributes:\n",
    "        - class priors\n",
    "        - mean for each class and each feature\n",
    "        - variance for each class and each feature\n",
    "\n",
    "        Args:\n",
    "            `X`: an array of shape (n_samples, n_features) containing feature values\n",
    "            `y`: an array of shape (n_samples,) containing true class labels\n",
    "        \n",
    "        Note:\n",
    "            Argument `y` is expected to be prepared such that\n",
    "            if there are n classes in total,\n",
    "            then `y` contains labels 0, 1, 2, ..., n-1.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        labels, counts = np.unique(y, return_counts=True)\n",
    "        self.n_classes = labels.size\n",
    "        self.priors = counts / n_samples # (n_classes,)\n",
    "        self.mean = np.zeros((self.n_classes, n_features))\n",
    "        self.var = np.zeros_like(self.mean) # (n_classes, n_features)\n",
    "        for label in labels:\n",
    "            mask = (y==label) # (n_samples,)\n",
    "            x = X[mask] # (local_n_samples, n_features)\n",
    "            assert x.shape[0] == counts[label]\n",
    "            self.mean[label] = x.mean(axis=0) # (n_features,)\n",
    "            self.var[label] = x.var(axis=0) # (n_features,)\n",
    "        max_var = self.var.max()\n",
    "        self.var += self.var_smoothing * max(1, max_var) # (n_classes, n_features)\n",
    "\n",
    "    def display_estimated_params(self):\n",
    "        \"\"\"\n",
    "        Displays estimated parameters of the model\n",
    "        \"\"\"\n",
    "        print(\"Estimated parameters:\")\n",
    "        print(f\"priors\\n{self.priors}\")\n",
    "        print(f\"means\\n{self.mean}\")\n",
    "        print(f\"variances\\n{self.var}\")\n",
    "\n",
    "    def predict(self, X: np.ndarray, return_proba: bool=False):\n",
    "        \"\"\"\n",
    "        Predicts labels of the input feature\n",
    "\n",
    "        If `return_proba` is True, also returns the predicted probabilities while\n",
    "\n",
    "        Args:\n",
    "            `X`: an array of shape (n_samples, n_features) containing feature values\n",
    "            `return_proba`: defaults to be False\n",
    "\n",
    "        Returns:\n",
    "            `y_pred`: an array of shape (n_samples,) containing predicted class labels\n",
    "            `proba`: an array of shape (n_samples, n_classes) containing predicted probabilities\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        X_aug = X.reshape(n_samples, 1, n_features)\n",
    "        mean_aug = self.mean.reshape(1, self.n_classes, n_features)\n",
    "        var_aug = self.var.reshape(1, self.n_classes, n_features)\n",
    "        log_priors = np.log(self.priors) # (n_classes,)\n",
    "        log_L = normal_log_pdf(X_aug, mean_aug, var_aug) # (n_samples, n_classes, n_features)\n",
    "        log_joint = log_priors + log_L.sum(axis=2) # (n_samples, n_classes)\n",
    "        y_pred = log_joint.argmax(axis=1) # (n_samples,)\n",
    "        if return_proba:\n",
    "            proba = self._get_proba(log_joint) # (n_samples, n_classes)\n",
    "            return y_pred, proba\n",
    "        return y_pred\n",
    "\n",
    "    def _get_proba(self, log_joint):\n",
    "        \"\"\"\n",
    "        Calculates predicted probabilities while storing `log_proba`\n",
    "\n",
    "        Returns:\n",
    "            `proba`: an array of shape (n_samples, n_classes) containing predicted probabilities\n",
    "        \"\"\"\n",
    "        C = log_joint.max()\n",
    "        joint_proba = np.exp(log_joint - C) # (n_samples, n_classes)\n",
    "        marginal = joint_proba.sum(axis=1) # (n_samples,)\n",
    "        proba = joint_proba / marginal.reshape(-1, 1) # (n_samples, n_classes)\n",
    "        return proba\n",
    "\n",
    "    def accuracy(self, X: np.ndarray, y: np.ndarray,\n",
    "                 return_pred: bool=False,\n",
    "                 return_proba: bool=False,\n",
    "                 return_loss: bool=False,):\n",
    "        \"\"\"\n",
    "        Calculates accuracy of prediction\n",
    "\n",
    "        If `return_pred` is True, also returns the predicted labels\n",
    "\n",
    "        If `return_proba` is True, also returns the predicted probabilities\n",
    "\n",
    "        If `return_loss` is True, also returns the cross entropy loss on the given dataset\n",
    "\n",
    "        Args:\n",
    "            `X`: an array of shape (n_samples, n_features) containing feature values\n",
    "            `y`: an array of shape (n_samples,) containing true class labels\n",
    "            `return_pred`: defaults to be False\n",
    "            `return_proba`: defaults to be False\n",
    "            `return_loss`: defaults to be False\n",
    "\n",
    "        Returns:\n",
    "            `accuracy`: a scalar that represents the proportion of points that are\n",
    "            predicted correctly\n",
    "            `y_pred`: an array of shape (n_samples,) containing predicted class labels\n",
    "            `proba`:  an array of shape (n_samples, n_classes) containing predicted probabilities\n",
    "            `loss`: a scalar that represents the cross entropy loss on the given dataset\n",
    "        \"\"\"\n",
    "        if not (return_proba or return_loss):\n",
    "            y_pred = self.predict(X)\n",
    "        else:\n",
    "            y_pred, proba = self.predict(X, return_proba=True)\n",
    "        accuracy = (y_pred==y).mean()\n",
    "        outputs = [accuracy.item()]\n",
    "        if return_pred:\n",
    "            outputs.append(y_pred)\n",
    "        if return_proba:\n",
    "            outputs.append(proba)\n",
    "        if return_loss:\n",
    "            loss = self._loss(y, proba)\n",
    "            outputs.append(loss)\n",
    "        return tuple(outputs)\n",
    "\n",
    "    def _loss(self, y_true: np.ndarray, proba: np.ndarray):\n",
    "        \"\"\"\n",
    "        Calculates the cross entropy loss given predicted probabilities and true labels\n",
    "\n",
    "        Args:\n",
    "            `y_true`: an array of shape (n_samples,) containing true class labels\n",
    "            `proba`: an array of shape (n_samples, n_classes) containing predicted probabilities\n",
    "\n",
    "        Returns:\n",
    "            `loss`: a scalar that represents the cross entropy loss\n",
    "        \"\"\"\n",
    "        n_samples = y_true.size\n",
    "        log_proba = np.log(proba)\n",
    "        loss = (-log_proba[np.arange(n_samples), y_true]).mean()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# fix random seed and state\n",
    "np.random.seed(42)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# instantiate our model and sklearn model\n",
    "model = GaussianNaiveBayes()\n",
    "clf = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test normal_log_pdf\n",
    "pdf_test_x = np.array([-1, 0, 1])\n",
    "pdf_test_mu = np.array([1, 2, 3])\n",
    "pdf_test_var = np.array([1, 4, 9])\n",
    "func_output = normal_log_pdf(pdf_test_x, pdf_test_mu, pdf_test_var)\n",
    "scipy_output = np.log(norm.pdf(pdf_test_x, pdf_test_mu, np.sqrt(pdf_test_var)))\n",
    "assert np.isclose(func_output, scipy_output).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model with a simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test simple case\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])\n",
    "X_pred = np.array([[-0.8, -1]])\n",
    "# train\n",
    "model.train(X, y)\n",
    "clf.fit(X, y)\n",
    "assert np.isclose(model.priors, clf.class_prior_).all()\n",
    "assert np.isclose(model.mean, clf.theta_).all()\n",
    "assert np.isclose(model.var, clf.var_).all()\n",
    "# predict\n",
    "model_y_pred = model.predict(X_pred)\n",
    "clf_y_pred = clf.predict(X_pred)\n",
    "assert (model_y_pred == clf_y_pred).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with an edge case where\n",
    "- there are two classes\n",
    "- values within each class and each feature are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test edge case\n",
    "X = np.array([[1, -1], [1, -1], [-1, 1], [-1, 1]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "X_pred = np.array([[-0.8, -1]])\n",
    "# train\n",
    "model.train(X, y)\n",
    "clf.fit(X, y)\n",
    "assert np.isclose(model.priors, clf.class_prior_).all()\n",
    "assert np.isclose(model.mean, clf.theta_).all()\n",
    "assert np.isclose(model.var, clf.var_).all()\n",
    "# predict\n",
    "model_y_pred = model.predict(X_pred)\n",
    "clf_y_pred = clf.predict(X_pred)\n",
    "assert (model_y_pred == clf_y_pred).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with an extreme edge case where\n",
    "- there is only one class\n",
    "- values within each feature are identical\n",
    "\n",
    "PS: the `sklearn` version reports `RunTimeWarning` here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/data2060/lib/python3.12/site-packages/sklearn/naive_bayes.py:510: RuntimeWarning: divide by zero encountered in log\n",
      "  n_ij = -0.5 * np.sum(np.log(2.0 * np.pi * self.var_[i, :]))\n",
      "/opt/anaconda3/envs/data2060/lib/python3.12/site-packages/sklearn/naive_bayes.py:511: RuntimeWarning: divide by zero encountered in divide\n",
      "  n_ij -= 0.5 * np.sum(((X - self.theta_[i, :]) ** 2) / (self.var_[i, :]), 1)\n",
      "/opt/anaconda3/envs/data2060/lib/python3.12/site-packages/sklearn/naive_bayes.py:511: RuntimeWarning: invalid value encountered in divide\n",
      "  n_ij -= 0.5 * np.sum(((X - self.theta_[i, :]) ** 2) / (self.var_[i, :]), 1)\n"
     ]
    }
   ],
   "source": [
    "# test extreme edge case\n",
    "X = np.array([[1, -1], [1, -1]])\n",
    "y = np.array([0, 0])\n",
    "X_pred = np.array([[-0.8, -1]])\n",
    "# train\n",
    "model.train(X, y)\n",
    "clf.fit(X, y)\n",
    "assert np.isclose(model.priors, clf.class_prior_).all()\n",
    "assert np.isclose(model.mean, clf.theta_).all()\n",
    "assert np.isclose(model.var, clf.var_).all()\n",
    "# predict\n",
    "model_y_pred = model.predict(X_pred)\n",
    "clf_y_pred = clf.predict(X_pred)\n",
    "assert (model_y_pred == clf_y_pred).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test performance on a generated large dataset that satisfies assumptions of Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for generating dataset:\n",
      "priors\n",
      "[0.2 0.5 0.3]\n",
      "means\n",
      "[[-2.50919762  9.01428613  4.63987884  1.97316968]\n",
      " [-6.87962719 -6.88010959 -8.83832776  7.32352292]\n",
      " [ 2.02230023  4.16145156 -9.58831011  9.39819704]]\n",
      "variances\n",
      "[[ 8.27924758 11.45358077 10.43864194  4.85481822]\n",
      " [ 6.84721222 10.67698813 14.97710084  8.98155417]\n",
      " [ 8.07097436 10.01565654  7.9168794   8.78782004]]\n",
      "Estimated parameters:\n",
      "priors\n",
      "[0.19969 0.50148 0.29883]\n",
      "means\n",
      "[[-2.51465312  9.02480353  4.68318818  1.95030105]\n",
      " [-6.87359249 -6.88080238 -8.83469895  7.30144095]\n",
      " [ 2.02503691  4.15016076 -9.61565662  9.42510813]]\n",
      "variances\n",
      "[[ 8.44881989 11.32674187 10.47666018  4.81761254]\n",
      " [ 6.86000106 10.70073827 14.89365958  8.96458058]\n",
      " [ 8.03148477 10.0843887   7.90376508  8.89021161]]\n",
      "Test accuracy by our model: 0.99292\n",
      "Test accuracy by sklearn model: 0.99292\n"
     ]
    }
   ],
   "source": [
    "# test with larger dataset\n",
    "def create_dataset(n_samples: int, prior: np.ndarray, params: np.ndarray):\n",
    "    \"\"\"\n",
    "    Generates a dataset that satisfies the assumptions of Gaussian Naive Bayes\n",
    "\n",
    "    Args:\n",
    "        `n_samples`: an integer indicating the size (# points) of the dataset\n",
    "        `prior`: an array of shape (n_classes,) containing the class prior\n",
    "        Note: `prior` should satisfy\n",
    "            (a) each entry is positive\n",
    "            (b) all entries sum up to 1\n",
    "        `params`: an array of shape (2, n_classes, n_features) containing\n",
    "        parameters of in-class normal distribution for each class and each feature\n",
    "        Note: the 2 entries of axis 0 of `params` respectively represent\n",
    "        mean and varaince of the normal distribution; all variances (represented by\n",
    "        `params[1]`) are expected to be positive\n",
    "\n",
    "    Returns:\n",
    "        `X`: an array of shape (n_samples, n_features) containing feature values\n",
    "        `y`: an array of shape (n_samples,) containing true class labels\n",
    "    \"\"\"\n",
    "    assert prior.ndim == 1\n",
    "    assert (prior > 0).all()\n",
    "    assert prior.sum() == 1\n",
    "    assert params.ndim == 3\n",
    "    n, n_class, n_features = params.shape\n",
    "    assert n_class == prior.shape[0]\n",
    "    assert n == 2\n",
    "    assert (params[1] > 0).all()\n",
    "    label_counts = np.random.multinomial(n_samples, pvals=prior)\n",
    "    X_list, y_list = [], []\n",
    "    for label, count in enumerate(label_counts):\n",
    "        y_list.append(label * np.ones(count, dtype=int))\n",
    "        mean = params[0, label]\n",
    "        var = params[1, label]\n",
    "        std = np.sqrt(var)\n",
    "        X_list.append(np.random.normal(mean, std, (count, n_features)))\n",
    "    X = np.concatenate(X_list, axis=0)\n",
    "    y = np.concatenate(y_list, axis=0)\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    X[:] = X[indices]\n",
    "    y[:] = y[indices]\n",
    "    return X, y\n",
    "\n",
    "# generate dataset and split\n",
    "test_prior = np.array([0.2, 0.5, 0.3])\n",
    "test_mean = np.random.uniform(-10.0, 10.0, (3, 4))\n",
    "test_var = np.random.gamma(10.0, size=(3, 4))\n",
    "print(\"Parameters for generating dataset:\")\n",
    "print(f\"priors\\n{test_prior}\\nmeans\\n{test_mean}\\nvariances\\n{test_var}\")\n",
    "test_params = np.stack((test_mean, test_var), axis=0)\n",
    "X, y = create_dataset(int(1.25e5), test_prior, test_params)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# our model\n",
    "model.train(X_train, y_train)\n",
    "model.display_estimated_params()\n",
    "acc, y_pred, proba, loss = model.accuracy(\n",
    "    X_test, y_test, return_pred=True, return_proba=True, return_loss=True)\n",
    "\n",
    "# sklearn model\n",
    "clf.fit(X_train, y_train)\n",
    "clf_y_pred = clf.predict(X_test)\n",
    "clf_proba = clf.predict_proba(X_test)\n",
    "clf_acc = clf.score(X_test, y_test)\n",
    "\n",
    "# test train\n",
    "assert np.isclose(model.priors, clf.class_prior_).all()\n",
    "assert np.isclose(model.mean, clf.theta_).all()\n",
    "assert np.isclose(model.var, clf.var_).all()\n",
    "\n",
    "# test predict\n",
    "assert (y_pred == clf_y_pred).all()\n",
    "assert np.isclose(proba, clf_proba).all()\n",
    "\n",
    "# print accuracies\n",
    "print(f\"Test accuracy by our model: {acc}\")\n",
    "print(f\"Test accuracy by sklearn model: {clf_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run both models on the wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy by our model: 1.0\n",
      "Test accuracy by sklearn model: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# prepare data\n",
    "column_names = [\n",
    "    'class',\n",
    "    'Alcohol',\n",
    "    'Malicacid',\n",
    "    'Ash',\n",
    "    'Alcalinity_of_ash',\n",
    "    'Magnesium',\n",
    "    'Total_phenols',\n",
    "    'Flavanoids',\n",
    "    'Nonflavanoid_phenols',\n",
    "    'Proanthocyanins',\n",
    "    'Color_intensity',\n",
    "    'Hue',\n",
    "    '0D280_0D315_of_diluted_wines',\n",
    "    'Proline',\n",
    "]\n",
    "df = pd.read_csv('../data/Wine.csv', names=column_names, header=None)\n",
    "y = df['class'].to_numpy()\n",
    "y -= 1\n",
    "X = df.loc[:, df.columns != 'class'].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# our model\n",
    "model.train(X_train, y_train)\n",
    "clf.fit(X_train, y_train)\n",
    "acc, y_pred = model.accuracy(\n",
    "    X_test, y_test, return_pred=True)\n",
    "\n",
    "# sklearn model\n",
    "clf_y_pred = clf.predict(X_test)\n",
    "clf_acc = clf.score(X_test, y_test)\n",
    "\n",
    "# test prediction\n",
    "assert (y_pred == clf_y_pred).all()\n",
    "\n",
    "# print accuracies\n",
    "print(f\"Test accuracy by our model: {acc}\")\n",
    "print(f\"Test accuracy by sklearn model: {clf_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
